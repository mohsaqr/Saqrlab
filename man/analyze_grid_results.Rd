% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analyze_grid_results.R
\name{analyze_grid_results}
\alias{analyze_grid_results}
\title{Analyze and Summarize Grid Simulation Results}
\usage{
analyze_grid_results(
  grid_results_list,
  num_rows_range = NULL,
  max_seq_length_range = NULL,
  min_na_range = NULL,
  max_na_range = NULL,
  level_context = 0.05,
  print_output = TRUE,
  print_aggregated_overall = TRUE,
  print_aggregated_edges = TRUE,
  print_settings_summary = TRUE
)
}
\arguments{
\item{grid_results_list}{List output from \code{run_grid_simulation()}.}

\item{num_rows_range}{Numeric vector of length 2 (min, max) or NULL.
Filter settings by num_rows.}

\item{max_seq_length_range}{Numeric vector of length 2 (min, max) or NULL.
Filter settings by max_seq_length.}

\item{min_na_range}{Numeric vector of length 2 (min, max) or NULL.
Filter settings by min_na.}

\item{max_na_range}{Numeric vector of length 2 (min, max) or NULL.
Filter settings by max_na.}

\item{level_context}{Numeric. Significance level used for calculations
(for context in output). Default: 0.05.}

\item{print_output}{Logical. Master switch for console printing. Default: TRUE.}

\item{print_aggregated_overall}{Logical. Print averaged overall performance
metrics across selected settings. Default: TRUE.}

\item{print_aggregated_edges}{Logical. Print aggregated edge significance
summary. Default: TRUE.}

\item{print_settings_summary}{Logical. Print summary table for each selected
setting. Default: TRUE.}
}
\value{
A list containing:
\describe{
\item{n_selected}{Number of settings matching the filter criteria.}
\item{aggregated_summary}{List with:
\itemize{
\item overall_performance: Averaged metrics across settings.
\item edge_significance: Aggregated edge-level statistics.
}
}
\item{selected_settings_summary_df}{Data frame with per-setting metrics
calculated from total TP/TN/FP/FN counts.}
\item{compiled_individual_runs}{List with detailed run-level data:
\itemize{
\item all_raw_summaries: Combined bootstrap summaries.
\item all_per_edge_performance: Combined per-edge results.
\item run_level_performance_metrics: Metrics per run.
\item setting_level_summary_stats: Mean/Median/SD of run metrics.
}
}
}
Returns NULL (invisibly) if no settings match.
}
\description{
Analyze results from \code{run_grid_simulation()}, filtering by parameter ranges
and computing aggregated performance metrics. Provides detailed summaries
at both the setting level and across all selected settings.
}
\details{
The function performs comprehensive analysis:

\strong{Filtering}: Selects settings where all parameters fall within specified ranges.

\strong{Aggregation from Input Summaries}: Extracts and averages overall performance
and edge significance from the original \code{aggregated_summary} in each setting.

\strong{Run-Level Metrics}: Recomputes TP/TN/FP/FN counts from per-edge data,
then calculates Sensitivity, Specificity, FPR, FNR, Accuracy, and MCC per run.

\strong{Setting-Level Metrics}: Two approaches:
\enumerate{
\item Mean/Median/SD of run-level metrics.
\item Metrics computed from total counts across all runs (more robust).
}

The \code{selected_settings_summary_df} uses approach #2 for the most accurate
overall metrics per setting.
}
\examples{
\dontrun{
# After running grid simulation
grid_results <- run_grid_simulation(...)

# Analyze all results
analysis <- analyze_grid_results(grid_results)

# Filter to specific parameter ranges
analysis_filtered <- analyze_grid_results(
  grid_results,
  num_rows_range = c(100, 200),
  max_seq_length_range = c(30, 50),
  max_na_range = c(0, 5)
)

# Access the detailed run-level metrics
run_metrics <- analysis$compiled_individual_runs$run_level_performance_metrics

# Access setting-level summary
setting_summary <- analysis$selected_settings_summary_df
}

}
